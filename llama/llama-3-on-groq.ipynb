{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from groq) (2.7.0)\n",
      "Requirement already satisfied: sniffio in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in d:\\generative_ai\\gen-ai-notebooks\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "model = \"llama3-70b-8192\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"what is Llama-3?\"\n",
    "chat_completion = groq_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    stream=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3 is an AI model developed by Meta AI that generates human-like text based on the input it receives. It's a type of language model that uses a large amount of text data to learn patterns and relationships in language, allowing it to understand and respond to user input in a conversational manner.\n",
      "\n",
      "Llama-3 is a significant improvement over its predecessors, with capabilities that include:\n",
      "\n",
      "1. **Conversational dialogue**: Llama-3 can engage in natural-sounding conversations, using context and understanding to respond to questions and statements.\n",
      "2. **Text generation**: It can generate human-like text on a given topic or subject, making it useful for tasks like content creation and writing assistance.\n",
      "3. **Question answering**: Llama-3 can process and respond to questions, providing accurate and relevant answers based on its vast knowledge base.\n",
      "4. **Language understanding**: It can comprehend nuances of language, including idioms, sarcasm, and figurative language, allowing it to better understand user input.\n",
      "\n",
      "Llama-3 has many potential applications, such as chatbots, virtual assistants, language translation tools, and content generation platforms. Its capabilities are continually improving, making it an exciting development in the field of natural language processing (NLP).\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
